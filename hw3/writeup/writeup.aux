\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Basic Q-Learning Performance}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces These results correspond to the default hyperparameters. It was run for 5 million timesteps on GPU.}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experimenting with Hyperparameters}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The plot aboves shows the effect of learning rate on various trials of the Q-Learning model on Pong with RAM for 1 million steps. I chose to experiment with tuning learning rate (learning rate multiplier) because I was interested in comparing the sensitivity of exploration on Q-Learning with the sensitivity of learning rate on other tasks (Q-Value Iteration, Behavior Cloning). My intuition was that high learning rates would perform badly, and that seems to be the case. I was surprised that a LR of 10 was able to learn very quickly at first, which suggests that perhaps a scheduler that starts very high and decays fast might perform well.}}{2}}
